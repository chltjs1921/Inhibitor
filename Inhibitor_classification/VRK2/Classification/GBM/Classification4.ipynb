{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "surprising-performer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 정확도: 1.0000\n",
      "GBM 수행시간: 0.2초\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼 파라미터:\n",
      " {}\n",
      "최고 예측 정확도: 0.7000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "#데이터 입력\n",
    "df = pd.read_csv('./VRK2_inhibitor_descriptor_Internal_dataset_editgbm.csv')\n",
    "\n",
    "#데이터 분류\n",
    "dataset = df.values\n",
    "X = dataset[:,0:-1]\n",
    "Y = dataset[:,-1]\n",
    "\n",
    "#X 표준화\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "#학습셋과 테스트셋을 나눔\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=10)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#모델 생성과 시행\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=10)\n",
    "gb_clf.fit(X_train, Y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(Y_test, gb_pred)\n",
    "\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
    "print('GBM 수행시간: {0:.1f}초'.format(time.time() - start_time))\n",
    "\n",
    "#파라미터 튜닝\n",
    "\n",
    "param = {}\n",
    "grid_clf = GridSearchCV(gb_clf, param_grid=param, cv=5, verbose=1)\n",
    "grid_clf.fit(X_train, Y_train)\n",
    "print('최적 하이퍼 파라미터:\\n',grid_clf.best_params_)\n",
    "print('최고 예측 정확도: {0:.4f}'.format(grid_clf.best_score_))\n",
    "\n",
    "#GBM 정확도: 0.8571\n",
    "#GBM 수행시간: 4.2초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "informational-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 내부 정확도: 1.0000\n",
      "[[ 6  0  0  0]\n",
      " [ 0  2  0  0]\n",
      " [ 0  0  8  0]\n",
      " [ 0  0  0 10]]\n",
      "[[0.23076923 0.         0.         0.        ]\n",
      " [0.         0.07692308 0.         0.        ]\n",
      " [0.         0.         0.30769231 0.        ]\n",
      " [0.         0.         0.         0.38461538]]\n",
      "GBM 외부 정확도: 0.3333\n",
      "[[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 2]\n",
      " [0 0 0 2]]\n",
      "[[0.         0.         0.         0.16666667]\n",
      " [0.         0.         0.         0.16666667]\n",
      " [0.         0.         0.         0.33333333]\n",
      " [0.         0.         0.         0.33333333]]\n",
      "GBM 전체 정확도: 0.8750\n",
      "[[ 6  0  0  1]\n",
      " [ 0  2  0  1]\n",
      " [ 0  0  8  2]\n",
      " [ 0  0  0 12]]\n",
      "[[0.1875  0.      0.      0.03125]\n",
      " [0.      0.0625  0.      0.03125]\n",
      " [0.      0.      0.25    0.0625 ]\n",
      " [0.      0.      0.      0.375  ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, cross_val_predict, KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix ,multilabel_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "#데이터 입력\n",
    "df = pd.read_csv('./VRK2_inhibitor_descriptor_Internal_dataset_editgbm.csv')\n",
    "df2 = pd.read_csv('./VRK2_inhibitor_descriptor_external_dataset_editgbm.csv')\n",
    "df3 = pd.concat([df, df2]) #total data는 internal과 external concat하면 혼동함수 만들 수 있다.\n",
    "\n",
    "#데이터 분류\n",
    "dataset = df.values\n",
    "X = dataset[:,0:-1]\n",
    "Y = dataset[:,-1]\n",
    "\n",
    "dataset2 = df2.values\n",
    "exX = dataset2[:,0:-1]\n",
    "exY = dataset2[:,-1]\n",
    "\n",
    "dataset3 = df3.values\n",
    "totX = dataset3[:,0:-1]\n",
    "totY = dataset3[:,-1]\n",
    "\n",
    "#X 표준화\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "exX_scaled = scaler.transform(exX)\n",
    "\n",
    "totX_scaled = scaler.transform(totX)\n",
    "\n",
    "#학습셋과 테스트셋을 나누고  crossval 실행\n",
    "#skf = KFold(n_splits=5, random_state=10)\n",
    "#n_iter = 0\n",
    "#cv_accuracy = []\n",
    "#cm_holder = []\n",
    "#gb_clf = GradientBoostingClassifier(random_state=10)\n",
    "\n",
    "#for train_index, test_index in skf.split(X_scaled, Y):\n",
    "#    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "#    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "#    gb_clf.fit(X_train, Y_train)\n",
    "#    pred = gb_clf.predict(X_test)\n",
    "    \n",
    "#    n_iter += 1\n",
    "#    accuracy = np.round(accuracy_score(Y_test, pred), 5)\n",
    "#    train_size = X_train.shape[0]\n",
    "#    test_size = X_test.shape[0]\n",
    "#    print('\\n#{0} 교차 검증 정확도 : {1}, 학습 데이터 크기 : {2}, 검증 데이터 크기 : {3}'.format(n_iter, accuracy, train_size, test_size))\n",
    "#    print('#{0} 검증 세트 인덱스 : {1}'.format(n_iter, test_index))\n",
    "#    print('#{0} 검증 세트 : {1}'.format(n_iter, Y_test))\n",
    "#    print('#{0} 검증 세트 예측 결과 : {1}'.format(n_iter, pred))\n",
    "#    cv_accuracy.append(accuracy)\n",
    "#    cm_holder.append(confusion_matrix(Y_test, pred, labels=[0,1,2,3]))\n",
    "\n",
    "#print('\\n## 교차 검증별 정확도:', np.round(cv_accuracy, 5))\n",
    "#print('## 평균 검증 정확도:', np.mean(cv_accuracy))\n",
    "#print(cm_holder)\n",
    "\n",
    "#검증이 끝난 모델을 저장하고 로드해서 conf matrix를 구한다.\n",
    "\n",
    "#학습셋과 테스트셋을 나눔\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=10)\n",
    "gb_clf.fit(X_train, Y_train)\n",
    "#start_time = time.time()\n",
    "\n",
    "#print('GBM 내부 정확도: {0:.4f}'.format(gb_accuracy))\n",
    "#print('GBM 수행시간: {0:.1f}초'.format(time.time() - start_time))\n",
    "\n",
    "#confision matrix   \n",
    "\n",
    "#conf_matrix1_1 = confusion_matrix(Y_test, gb_pred1,labels=[0,1,2,3])\n",
    "#print(conf_matrix1_1)\n",
    "#conf_matrix1_2 = confusion_matrix(Y_test2, gb_pred2,labels=[0,1,2,3])\n",
    "#print(conf_matrix1_2)\n",
    "#conf_matrix1_3 = confusion_matrix(Y_test3, gb_pred3,labels=[0,1,2,3])\n",
    "#print(conf_matrix1_3)\n",
    "#conf_matrix1_4 = confusion_matrix(Y_test4, gb_pred4,labels=[0,1,2,3])\n",
    "#print(conf_matrix1_4)\n",
    "#conf_matrix1_5 = confusion_matrix(Y_train4, gb_pred5,labels=[0,1,2,3])\n",
    "#print(conf_matrix1_5)\n",
    "#conf_matrix1 = conf_matrix1_1+ conf_matrix1_2 + conf_matrix1_3 + conf_matrix1_4 + conf_matrix1_5\n",
    "#print(conf_matrix1)\n",
    "\n",
    "pred = gb_clf.predict(X)\n",
    "accuracy = accuracy_score(Y, pred)\n",
    "print('GBM 내부 정확도: {0:.4f}'.format(accuracy))\n",
    "conf_matrix = confusion_matrix(Y, pred,labels=[0,1,2,3])\n",
    "print(conf_matrix)\n",
    "conf_matrix_nor = confusion_matrix(Y, pred,labels=[0,1,2,3])/26\n",
    "print(conf_matrix_nor)\n",
    "\n",
    "pred2 = gb_clf.predict(exX)\n",
    "accuracy2 = accuracy_score(exY, pred2)\n",
    "print('GBM 외부 정확도: {0:.4f}'.format(accuracy2))\n",
    "conf_matrix2 = confusion_matrix(exY, pred2,labels=[0,1,2,3])\n",
    "print(conf_matrix2)\n",
    "conf_matrix2_nor = confusion_matrix(exY, pred2,labels=[0,1,2,3])/6\n",
    "print(conf_matrix2_nor)\n",
    "\n",
    "pred3 = gb_clf.predict(totX)\n",
    "accuracy3 = accuracy_score(totY, pred3)\n",
    "print('GBM 전체 정확도: {0:.4f}'.format(accuracy3))\n",
    "conf_matrix3 = confusion_matrix(totY, pred3,labels=[0,1,2,3])\n",
    "print(conf_matrix3)\n",
    "conf_matrix3_nor = confusion_matrix(totY, pred3,labels=[0,1,2,3])/32\n",
    "print(conf_matrix3_nor)\n",
    "\n",
    "#fig, ax = plt.subplots(nrows = 3, ncols = 2, figsize=(15, 20))\n",
    "#sns.heatmap(conf_matrix, annot=True, ax=ax[0][0])\n",
    "#ax[0][0].set_title('Internal confusion matrix', pad = 12)\n",
    "#sns.heatmap(conf_matrix_nor, annot=True, ax=ax[1][0])\n",
    "#ax[1][0].set_title('Internal confusion matrix(Normalized)', pad = 12)\n",
    "#sns.heatmap(conf_matrix2, annot=True, ax=ax[0][1])\n",
    "#ax[0][1].set_title('External confusion matrix', pad = 12)\n",
    "#sns.heatmap(conf_matrix2_nor, annot=True, ax=ax[1][1])\n",
    "#ax[1][1].set_title('External confusion matrix(Normalized)', pad = 12)\n",
    "#sns.heatmap(conf_matrix3, annot=True, ax=ax[2][0])\n",
    "#ax[2][0].set_title('Total confusion matrix', pad = 12)\n",
    "#sns.heatmap(conf_matrix3_nor, annot=True, ax=ax[2][1])\n",
    "#ax[2][1].set_title('Total confusion matrix(Normalized)', pad = 12)\n",
    "\n",
    "#모델 저장\n",
    "#saved_model_rf = joblib.dump(gb_clf,'./VRK2_GBM.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "alpha-january",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 정확도: 0.8333\n",
      "GBM 수행시간: 0.4초\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#데이터 입력\n",
    "df = pd.read_csv('./VRK2_inhibitor_descriptor_Internal_dataset_editgbm.csv')\n",
    "\n",
    "#데이터 분류\n",
    "dataset = df.values\n",
    "X = dataset[:,0:-1]\n",
    "Y = dataset[:,-1]\n",
    "\n",
    "#X 정규화\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "#학습셋과 테스트셋을 나눔\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=4)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#모델 생성과 시행\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train, Y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(Y_test, gb_pred)\n",
    "\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
    "print('GBM 수행시간: {0:.1f}초'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "#importance = gb_clf.feature_importances_\n",
    "\n",
    "# summarize feature importance\n",
    "\n",
    "#sys.stdout = open('class4_nonzero.txt', 'w') \n",
    "\n",
    "#for i,v in enumerate(importance):\n",
    "#    print('%0d,%.5f' % (i,v))\n",
    "    \n",
    "#sys.stdout.close()      \n",
    "       \n",
    "# plot feature importance\n",
    "#plt.bar([x for x in range(len(importance))], importance)\n",
    "#plt.axis([1725, 1800, 0, 0.01])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cutting-physiology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.arange(0.1, 0.5, 0.05)\n",
    "\n",
    "#list(range(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spanish-inspector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 정확도: 0.6667\n",
      "GBM 수행시간: 0.0초\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#데이터 입력\n",
    "df = pd.read_csv('./VRK2_inhibitor_descriptor_Internal_dataset_editgbm.csv')\n",
    "\n",
    "#데이터 분류\n",
    "dataset = df.values\n",
    "X = dataset[:,0:-1]\n",
    "Y = dataset[:,-1]\n",
    "\n",
    "#학습셋과 테스트셋을 나눔\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#모델 생성과 시행\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(learning_rate=0.19, max_depth=1, min_samples_leaf=3, min_samples_split=2, \n",
    "                                    n_estimators=4, subsample=0.5, random_state=10)\n",
    "gb_clf.fit(X_train, Y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(Y_test, gb_pred)\n",
    "\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
    "print('GBM 수행시간: {0:.1f}초'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fundamental-millennium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-session",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
